# 📘 Fundamentals of Software Engineering for Data

Welcome to the **Fundamentals of Software Engineering for Data** course repository! This course is designed for beginners who are pursuing a career in **data science, data analytics**, or **machine learning**, and provides the essential software engineering skills to build strong technical foundations.

---

## 📚 Course Overview

This hands-on course introduces key tools and concepts widely used in the data ecosystem, including:

- 🐧 GNU/Linux and Shell Scripting  
- 🐍 Python Programming (standard & third-party libraries)  
- 🗃️ Databases and SQL (SQLite, SQLAlchemy)  
- 🌐 Web APIs using FastAPI  
- 🧪 Git & GitHub for version control  
- 🧱 dbt for analytics engineering  
- 🐳 Docker and Docker Compose for containerization  

Each lesson includes **theory, lab exercises**, and **mini-projects** to reinforce practical understanding.

---

## 🧠 Prerequisites

**None.** This course is beginner-friendly and welcomes all learners interested in building software skills for data-related roles.

---

## 🧑‍🏫 Instructor

**Tien Nguyen** – Machine Learning Engineer at [MoMo](https://momo.vn) with over 3 years of experience in data and machine learning.

---

## 🖥️ System Requirements

- Minimum **16GB RAM**
- At least **150GB free disk space**
- OS: Windows, macOS, or Linux

---

## 📆 Schedule & Learning Mode

- 📍 **Live sessions via Zoom**: Every **Tuesday & Thursday**
- 🎥 **Recordings** will be shared after each session

---

## 🧪 Curriculum Breakdown

| Module              | Topics Covered                                                                                                                                       | Lab Activities                                                                                          |
|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| **Linux**           | GNU/Linux basics, bash scripting, Makefile, CronJob                                                                                                   | Build a CLI tool to crawl & collect text from websites                                                  |
| **Python I**        | Syntax, data types, loops, functions, file handling                                                                                                   | Analyze word frequency in crawled documents                                                             |
| **Python II**       | Standard libraries, pip, argparse, error handling, regex                                                                                              | Enhance word counter using Regex and NLTK                                                               |
| **Database & SQL**  | SQL basics, SQLite, SQLAlchemy                                                                                                                        | Save TF-IDF results to a database                                                                       |
| **WebAPI I**        | FastAPI, HTTP methods, Swagger, Postman, Locust                                                                                                       | Build an API for classifying documents                                                                  |
| **WebAPI II**       | Pydantic, error handling, health checks, logging                                                                                                      | Improve the API with response models and monitoring                                                     |
| **Git & GitHub**    | Git basics, GitHub workflow, resolving conflicts, collaboration best practices                                                                        | Track and version your API using Git and GitHub                                                         |
| **dbt**             | Setup, models, materializations, pipelines, testing, documentation                                                                                   | Rebuild workflow using dbt and SQL                                                                      |
| **Docker**          | Dockerfile, building/running containers, PostgreSQL in Docker                                                                                         | Containerize API and connect to PostgreSQL                                                              |
| **Docker Compose**  | Compose file, multi-container setup, environment config, volume persistence                                                                           | Define and run API + PostgreSQL stack using Docker Compose                                              |

---

## 🚀 Outcomes

By the end of this course, you will:

✅ Understand key software tools used in data workflows  
✅ Be able to build, version, and deploy real-world data applications  
✅ Have hands-on experience with APIs, databases, containers, and pipelines  
✅ Be ready to take on more advanced topics in data science and machine learning

---

## 🧭 Getting Started

1
